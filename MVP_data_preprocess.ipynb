{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRScraper import nba\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 multi-team player rows.\n",
      "Processed 1980-81 successfully!\n",
      "Found 28 multi-team player rows.\n",
      "Processed 1981-82 successfully!\n",
      "Found 36 multi-team player rows.\n",
      "Processed 1982-83 successfully!\n",
      "Found 15 multi-team player rows.\n",
      "Processed 1983-84 successfully!\n",
      "Found 20 multi-team player rows.\n",
      "Processed 1984-85 successfully!\n",
      "Found 25 multi-team player rows.\n",
      "Processed 1985-86 successfully!\n",
      "Found 21 multi-team player rows.\n",
      "Processed 1986-87 successfully!\n",
      "Found 46 multi-team player rows.\n",
      "Processed 1987-88 successfully!\n",
      "Found 42 multi-team player rows.\n",
      "Processed 1988-89 successfully!\n",
      "Found 38 multi-team player rows.\n",
      "Processed 1989-90 successfully!\n",
      "Found 26 multi-team player rows.\n",
      "Processed 1990-91 successfully!\n",
      "Found 33 multi-team player rows.\n",
      "Processed 1991-92 successfully!\n",
      "Found 28 multi-team player rows.\n",
      "Processed 1992-93 successfully!\n",
      "Found 37 multi-team player rows.\n",
      "Processed 1993-94 successfully!\n",
      "Found 22 multi-team player rows.\n",
      "Processed 1994-95 successfully!\n",
      "Found 56 multi-team player rows.\n",
      "Processed 1995-96 successfully!\n",
      "Found 63 multi-team player rows.\n",
      "Processed 1996-97 successfully!\n",
      "Found 53 multi-team player rows.\n",
      "Processed 1997-98 successfully!\n",
      "Found 33 multi-team player rows.\n",
      "Processed 1998-99 successfully!\n",
      "Found 28 multi-team player rows.\n",
      "Processed 1999-00 successfully!\n",
      "Found 47 multi-team player rows.\n",
      "Processed 2000-01 successfully!\n",
      "Found 30 multi-team player rows.\n",
      "Processed 2001-02 successfully!\n",
      "Found 27 multi-team player rows.\n",
      "Processed 2002-03 successfully!\n",
      "Found 68 multi-team player rows.\n",
      "Processed 2003-04 successfully!\n",
      "Found 59 multi-team player rows.\n",
      "Processed 2004-05 successfully!\n",
      "Found 51 multi-team player rows.\n",
      "Processed 2005-06 successfully!\n",
      "Found 29 multi-team player rows.\n",
      "Processed 2006-07 successfully!\n",
      "Found 68 multi-team player rows.\n",
      "Processed 2007-08 successfully!\n",
      "Found 67 multi-team player rows.\n",
      "Processed 2008-09 successfully!\n",
      "Found 66 multi-team player rows.\n",
      "Processed 2009-10 successfully!\n",
      "Found 83 multi-team player rows.\n",
      "Processed 2010-11 successfully!\n",
      "Found 36 multi-team player rows.\n",
      "Processed 2011-12 successfully!\n",
      "Found 50 multi-team player rows.\n",
      "Processed 2012-13 successfully!\n",
      "Found 63 multi-team player rows.\n",
      "Processed 2013-14 successfully!\n",
      "Found 76 multi-team player rows.\n",
      "Processed 2014-15 successfully!\n",
      "Found 50 multi-team player rows.\n",
      "Processed 2015-16 successfully!\n",
      "Found 53 multi-team player rows.\n",
      "Processed 2016-17 successfully!\n",
      "Found 59 multi-team player rows.\n",
      "Processed 2017-18 successfully!\n",
      "Found 86 multi-team player rows.\n",
      "Processed 2018-19 successfully!\n",
      "Found 60 multi-team player rows.\n",
      "Processed 2019-20 successfully!\n",
      "Found 79 multi-team player rows.\n",
      "Processed 2020-21 successfully!\n",
      "Found 97 multi-team player rows.\n",
      "Processed 2021-22 successfully!\n",
      "Found 70 multi-team player rows.\n",
      "Processed 2022-23 successfully!\n",
      "Found 78 multi-team player rows.\n",
      "Processed 2023-24 successfully!\n",
      "No multi-team players found in the dataset.\n",
      "Processed 2024-25 successfully!\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess a single season\n",
    "def preprocess_season(file_name, year):\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "# Handle players who played for multiple teams\n",
    "    # Create a dictionary to store multi-team player data (including players with 2TM, 3TM, etc.)\n",
    "    multi_team_dict = {}\n",
    "\n",
    "    # Identify players with 'TM' in their 'Team' column (multi-team players)\n",
    "    multi_team_rows = df[df['Team'].str.contains('TM', na=False)]\n",
    "\n",
    "    # Check if there are any multi-team players\n",
    "    if multi_team_rows.empty:\n",
    "        print(\"No multi-team players found in the dataset.\")\n",
    "    else:\n",
    "        print(f\"Found {len(multi_team_rows)} multi-team player rows.\")\n",
    "\n",
    "    # For each multi-team player, gather all teams they played for (excluding 2TM, 3TM, etc.)\n",
    "    for player in multi_team_rows['Player'].unique():\n",
    "        player_teams = df[(df['Player'] == player) & ~df['Team'].str.contains('TM', na=False)]['Team'].tolist()\n",
    "        multi_team_dict[player] = ', '.join(player_teams)\n",
    "\n",
    "    # Remove duplicate rows for multi-team players and keep the rows with 'TM' in the Team column\n",
    "    multi_team_players = multi_team_rows['Player'].unique()\n",
    "    mask = (df['Team'].str.contains('TM', na=False)) | (~df['Player'].isin(multi_team_players))\n",
    "    df = df[mask]\n",
    "\n",
    "    # Add the 'Multiple Teams' column using the mapping from the multi_team_dict\n",
    "    df['Multiple Teams'] = df['Player'].map(multi_team_dict)\n",
    "\n",
    "    # Fill NaN values in the 'Multiple Teams' column with an empty string\n",
    "    df.fillna({'Multiple Teams': ''}, inplace=True)\n",
    "\n",
    "    # Reorder columns to place 'Multiple Teams' between 'Team' and 'Pos'\n",
    "    columns = list(df.columns)\n",
    "    if 'Multiple Teams' in columns and 'Team' in columns:\n",
    "        team_index = columns.index('Team')  # Get the index of 'Team' column\n",
    "        columns.insert(team_index + 1, columns.pop(columns.index('Multiple Teams')))  # Reorder columns\n",
    "        df = df[columns]\n",
    "\n",
    "\n",
    "\n",
    "# Addition of the TS% feature\n",
    "    # Drop unnecessary columns\n",
    "    drop_columns = ['Age', 'Pos', 'GS', '3PA', '2PA', 'PF']\n",
    "    df_cleaned = df.drop(columns=drop_columns, errors='ignore')\n",
    "\n",
    "    # Calculate TS%\n",
    "    if 'PTS' in df_cleaned.columns and 'FGA' in df_cleaned.columns and 'FTA' in df_cleaned.columns:\n",
    "        df_cleaned['TS%'] = df_cleaned['PTS'] / (2 * (df_cleaned['FGA'] + 0.44 * df_cleaned['FTA']))\n",
    "        df_cleaned['TS%'] = df_cleaned['TS%'].round(2)\n",
    "\n",
    "\n",
    "# Addition of the EFF feature\n",
    "    # Calculate missed shots for EFF\n",
    "    if 'FGA' in df_cleaned.columns and 'FG' in df_cleaned.columns:\n",
    "        df_cleaned['Missed_FG'] = df_cleaned['FGA'] - df_cleaned['FG']\n",
    "    if 'FTA' in df_cleaned.columns and 'FT' in df_cleaned.columns:\n",
    "        df_cleaned['Missed_FT'] = df_cleaned['FTA'] - df_cleaned['FT']\n",
    "\n",
    "    # Calculate EFF\n",
    "    if {'PTS', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'G', 'Missed_FG', 'Missed_FT'}.issubset(df_cleaned.columns):\n",
    "        df_cleaned['EFF'] = (\n",
    "            df_cleaned['PTS'] +\n",
    "            df_cleaned['TRB'] +\n",
    "            df_cleaned['AST'] +\n",
    "            df_cleaned['STL'] +\n",
    "            df_cleaned['BLK'] -\n",
    "            df_cleaned['Missed_FG'] -\n",
    "            df_cleaned['Missed_FT'] -\n",
    "            df_cleaned['TOV']\n",
    "        ) / df_cleaned['G']\n",
    "        df_cleaned['EFF'] = df_cleaned['EFF'].round(2)\n",
    "\n",
    "    # Drop temporary columns\n",
    "    df_cleaned.drop(columns=['Missed_FG', 'Missed_FT'], inplace=True, errors='ignore')\n",
    "    \n",
    "    \n",
    "    \n",
    "# Addition of the MVP feature, ROY feature, AS feature, and All-NBA feature\n",
    "    # Define award categories\n",
    "    mvp_awards = [f'MVP-{i}' for i in range(1, 11)]\n",
    "    dpoy_awards = [f'DPOY-{i}' for i in range(1, 11)]\n",
    "    six_moy_awards = [f'6MOY-{i}' for i in range(1, 6)]\n",
    "    roy_awards = [f'ROY-{i}' for i in range(1, 6)]\n",
    "    all_nba_awards = ['NBA1', 'NBA2', 'NBA3']\n",
    "    as_awards = ['AS']\n",
    "    \n",
    "    # Initialize new columns with empty strings\n",
    "    df_cleaned['MVP'] = ''\n",
    "    df_cleaned['DPOY'] = ''\n",
    "    df_cleaned['6MOY'] = ''\n",
    "    df_cleaned['ROY'] = ''\n",
    "    df_cleaned['AS'] = ''\n",
    "    df_cleaned['All-NBA'] = ''\n",
    "\n",
    "    # Function to extract awards based on categories\n",
    "    def extract_awards(awards_string, category_list):\n",
    "        if pd.isna(awards_string):\n",
    "            return ''\n",
    "        awards = [award.strip() for award in awards_string.split(',')]\n",
    "        filtered_awards = [award for award in awards if award in category_list]\n",
    "        return ','.join(filtered_awards)\n",
    "\n",
    "    # Apply the function to split awards into respective columns\n",
    "    df_cleaned['MVP'] = df_cleaned['Awards'].apply(lambda x: extract_awards(x, mvp_awards))\n",
    "    df_cleaned['DPOY'] = df_cleaned['Awards'].apply(lambda x: extract_awards(x, dpoy_awards))\n",
    "    df_cleaned['6MOY'] = df_cleaned['Awards'].apply(lambda x: extract_awards(x, six_moy_awards))\n",
    "    df_cleaned['ROY'] = df_cleaned['Awards'].apply(lambda x: extract_awards(x, roy_awards))\n",
    "    df_cleaned['AS'] = df_cleaned['Awards'].apply(lambda x: extract_awards(x, as_awards))\n",
    "    df_cleaned['All-NBA'] = df_cleaned['Awards'].apply(lambda x: extract_awards(x, all_nba_awards))\n",
    "\n",
    "    # One-hot encode 'AS' (All-Star) feature\n",
    "    df_cleaned['AS'] = df_cleaned['AS'].apply(lambda x: 1 if x == 'AS' else 0)\n",
    "\n",
    "    # Apply numeric values for 'MVP', 'ROY', and 'All-NBA' values\n",
    "    # MVP - Using lambda to assign numbers based on MVP-1, MVP-2, ..., MVP-10\n",
    "    df_cleaned['MVP'] = df_cleaned['MVP'].apply(lambda x: 10 if 'MVP-10' in str(x) else\n",
    "                                        (2 if 'MVP-2' in str(x) else\n",
    "                                         (3 if 'MVP-3' in str(x) else\n",
    "                                          (4 if 'MVP-4' in str(x) else\n",
    "                                           (5 if 'MVP-5' in str(x) else\n",
    "                                            (6 if 'MVP-6' in str(x) else\n",
    "                                             (7 if 'MVP-7' in str(x) else\n",
    "                                              (8 if 'MVP-8' in str(x) else\n",
    "                                               (9 if 'MVP-9' in str(x) else\n",
    "                                                (1 if 'MVP-1' in str(x) else 0))))))))))\n",
    "    \n",
    "    # DPOY - Using lambda to assign numbers based on DPOY-1, DPOY-2, ..., DPOY-10\n",
    "    df_cleaned['DPOY'] = df_cleaned['DPOY'].apply(lambda x: 10 if 'DPOY-10' in str(x) else\n",
    "                                        (2 if 'DPOY-2' in str(x) else\n",
    "                                         (3 if 'DPOY-3' in str(x) else\n",
    "                                          (4 if 'DPOY-4' in str(x) else\n",
    "                                           (5 if 'DPOY-5' in str(x) else\n",
    "                                            (6 if 'DPOY-6' in str(x) else\n",
    "                                             (7 if 'DPOY-7' in str(x) else\n",
    "                                              (8 if 'DPOY-8' in str(x) else\n",
    "                                               (9 if 'DPOY-9' in str(x) else\n",
    "                                                (1 if 'DPOY-1' in str(x) else 0))))))))))\n",
    "\n",
    "    # 6MOY - Using lambda to assign numbers based on 6MOY-1, 6MOY-2, ..., 6MOY-5\n",
    "    df_cleaned['6MOY'] = df_cleaned['6MOY'].apply(lambda x: 1 if '6MOY-1' in str(x) else\n",
    "                                         (2 if '6MOY-2' in str(x) else\n",
    "                                          (3 if '6MOY-3' in str(x) else\n",
    "                                           (4 if '6MOY-4' in str(x) else\n",
    "                                            (5 if '6MOY-7' in str(x) else 0)))))\n",
    "\n",
    "    # ROY - Using lambda to assign numbers based on ROY-1, ROY-2, ..., ROY-5\n",
    "    df_cleaned['ROY'] = df_cleaned['ROY'].apply(lambda x: 1 if 'ROY-1' in str(x) else\n",
    "                                         (2 if 'ROY-2' in str(x) else\n",
    "                                          (3 if 'ROY-3' in str(x) else\n",
    "                                           (4 if 'ROY-4' in str(x) else\n",
    "                                            (5 if 'ROY-7' in str(x) else 0)))))\n",
    "\n",
    "    # All-NBA - Using lambda to assign numbers based on NBA1, NBA2\n",
    "    df_cleaned['All-NBA'] = df_cleaned['All-NBA'].apply(lambda x: 1 if 'NBA1' in str(x) else\n",
    "                                            (2 if 'NBA2' in str(x) else\n",
    "                                             (3 if 'NBA3' in str(x) else 0)))\n",
    "\n",
    "    \n",
    "\n",
    "# Addition of the MVP_count feature\n",
    "    # Add MVP_count column\n",
    "    df_cleaned['MVP_count'] = 0\n",
    "    mvp_df = pd.read_csv('nba_player_stats_mvp_data.csv')\n",
    "\n",
    "    # Extract the starting year from the 'season' column in mvp_df\n",
    "    mvp_df['season_start'] = mvp_df['Season'].str.split('-').str[0].astype(int)\n",
    "\n",
    "    # Filter MVP data for seasons before the current year\n",
    "    prior_mvp_df = mvp_df[mvp_df['season_start'] < year]\n",
    "\n",
    "    # Count MVP wins per player\n",
    "    mvp_counts = prior_mvp_df['Player'].value_counts().to_dict()  # {player_name: mvp_count}\n",
    "\n",
    "    # Map MVP counts to the current season's players\n",
    "    df_cleaned['MVP_count'] = df_cleaned['Player'].map(mvp_counts).fillna(0).astype(int)\n",
    "    \n",
    "    \n",
    "# Addition of the MVP_nominations feature\n",
    "    # Add MVP_nominations column\n",
    "    df_cleaned['MVP_nominations'] = 0\n",
    "    \n",
    "    # Cumulative nomination tracker: {player_name: total_nominations}\n",
    "    cumulative_nominations = {}\n",
    "     \n",
    "    for each_year in range(year - 1, 1979, -1):\n",
    "        current_season = f\"{each_year}-{str(each_year+1)[-2:]}\"\n",
    "        file_in = f\"{\"untouched_seasonal_data\"}/nba_player_stats_{current_season}.csv\"\n",
    "        \n",
    "        if os.path.exists(file_in):\n",
    "            this_df = pd.read_csv(file_in)\n",
    "            \n",
    "            # Check the awards column for MVP nominations (mvp-1 to mvp-10)\n",
    "            if 'Awards' in this_df.columns:\n",
    "                for _, row in this_df.iterrows():\n",
    "                    player = row['Player']\n",
    "                    awards = str(row.get('Awards', ''))  # Get the awards column, default to an empty string\n",
    "                \n",
    "                    # Check if the player was nominated for MVP\n",
    "                    if any(f\"mvp-{i}\" in awards.lower() for i in range(1, 11)):\n",
    "                        # Increment cumulative nominations for the player\n",
    "                        cumulative_nominations[player] = cumulative_nominations.get(player, 0) + 1\n",
    "            \n",
    "    # Add the cumulative nominations from prior years to the current dataset\n",
    "    df_cleaned['MVP_nominations'] = df_cleaned['Player'].map(cumulative_nominations).fillna(0).astype(int)\n",
    "\n",
    "         \n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the folder containing the CSV files and desired output\n",
    "input_folder = \"untouched_seasonal_data\"\n",
    "output_folder = \"processed_data\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for year in range(1980, 2025):\n",
    "    season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "    file_name = f\"{input_folder}/nba_player_stats_{season}.csv\"\n",
    "    output_file = f\"{output_folder}/nba_player_stats_{season}_processed.csv\"    \n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            processed_df = preprocess_season(file_name, year)\n",
    "            processed_df.to_csv(output_file, index=False)\n",
    "            print(f\"Processed {season} successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {season}: {e}\")\n",
    "    else:\n",
    "        print(f\"File {file_name} not found. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocessed the data to include two new features to help predict the MVP Ranking from 1-10. all other players will have NaN. From the new preprocessed datasets for each year from 1980 to 2023 we can create models to predict MVP Rank for each play and compare for each model how well it predicts to actual winners for each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for the 1980 season\n",
      "Successfully read data for the 1981 season\n",
      "Successfully read data for the 1982 season\n",
      "Successfully read data for the 1983 season\n",
      "Successfully read data for the 1984 season\n",
      "Successfully read data for the 1985 season\n",
      "Successfully read data for the 1986 season\n",
      "Successfully read data for the 1987 season\n",
      "Successfully read data for the 1988 season\n",
      "Successfully read data for the 1989 season\n",
      "Successfully read data for the 1990 season\n",
      "Successfully read data for the 1991 season\n",
      "Successfully read data for the 1992 season\n",
      "Successfully read data for the 1993 season\n",
      "Successfully read data for the 1994 season\n",
      "Successfully read data for the 1995 season\n",
      "Successfully read data for the 1996 season\n",
      "Successfully read data for the 1997 season\n",
      "Successfully read data for the 1998 season\n",
      "Successfully read data for the 1999 season\n",
      "Successfully read data for the 2000 season\n",
      "Successfully read data for the 2001 season\n",
      "Successfully read data for the 2002 season\n",
      "Successfully read data for the 2003 season\n",
      "Successfully read data for the 2004 season\n",
      "Successfully read data for the 2005 season\n",
      "Successfully read data for the 2006 season\n",
      "Successfully read data for the 2007 season\n",
      "Successfully read data for the 2008 season\n",
      "Successfully read data for the 2009 season\n",
      "Successfully read data for the 2010 season\n",
      "Successfully read data for the 2011 season\n",
      "Successfully read data for the 2012 season\n",
      "Successfully read data for the 2013 season\n",
      "Successfully read data for the 2014 season\n",
      "Successfully read data for the 2015 season\n",
      "All datasets have been concatenated and saved to 'nba_combined_1980_2015.csv'.\n"
     ]
    }
   ],
   "source": [
    "#Combine all datasets from 1980 to 2015\n",
    "# Define the folder containing the CSV files\n",
    "input_folder = \"processed_data\"\n",
    "\n",
    "# List to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Loop through each year from 1980 to 2015\n",
    "for year in range(1980, 2016):\n",
    "    season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "    file_name = f\"{input_folder}/nba_player_stats_{season}_processed.csv\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            # Read the dataset for the year\n",
    "            df = pd.read_csv(file_name)\n",
    "        \n",
    "            # Append the dataframe to the list\n",
    "            dataframes.append(df)\n",
    "        \n",
    "            print(f\"Successfully read data for the {year} season\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read data for {year} season. Error: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file\n",
    "combined_df.to_csv(\"nba_combined_1980_2015.csv\", index=False)\n",
    "\n",
    "print(\"All datasets have been concatenated and saved to 'nba_combined_1980_2015.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for the 2016 season\n",
      "Successfully read data for the 2017 season\n",
      "Successfully read data for the 2018 season\n",
      "Successfully read data for the 2019 season\n",
      "Successfully read data for the 2020 season\n",
      "Successfully read data for the 2021 season\n",
      "Successfully read data for the 2022 season\n",
      "Successfully read data for the 2023 season\n",
      "Successfully read data for the 2024 season\n",
      "All datasets have been concatenated and saved to 'nba_combined_2016_2024.csv'.\n"
     ]
    }
   ],
   "source": [
    "#Combine all datasets from 2016 to 2024\n",
    "# Define the folder containing the CSV files\n",
    "input_folder = \"processed_data\"\n",
    "\n",
    "# List to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Loop through each year from 2016 to 2024\n",
    "for year in range(2016, 2025):\n",
    "    season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "    file_name = f\"{input_folder}/nba_player_stats_{season}_processed.csv\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            # Read the dataset for the year\n",
    "            df = pd.read_csv(file_name)\n",
    "        \n",
    "            # Append the dataframe to the list\n",
    "            dataframes.append(df)\n",
    "        \n",
    "            print(f\"Successfully read data for the {year} season\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read data for {year} season. Error: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file\n",
    "combined_df.to_csv(\"nba_combined_2016_2024.csv\", index=False)\n",
    "\n",
    "print(\"All datasets have been concatenated and saved to 'nba_combined_2016_2024.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved standings for 1980-81\n",
      "Saved standings for 1981-82\n",
      "Saved standings for 1982-83\n",
      "Saved standings for 1983-84\n",
      "Saved standings for 1984-85\n",
      "Saved standings for 1985-86\n",
      "Saved standings for 1986-87\n",
      "Saved standings for 1987-88\n",
      "Saved standings for 1988-89\n",
      "Saved standings for 1989-90\n",
      "Saved standings for 1990-91\n",
      "Saved standings for 1991-92\n",
      "Saved standings for 1992-93\n",
      "Saved standings for 1993-94\n",
      "Saved standings for 1994-95\n",
      "Saved standings for 1995-96\n",
      "Saved standings for 1996-97\n",
      "Saved standings for 1997-98\n",
      "Saved standings for 1998-99\n",
      "Saved standings for 1999-00\n",
      "Saved standings for 2000-01\n",
      "Saved standings for 2001-02\n",
      "Saved standings for 2002-03\n",
      "Saved standings for 2003-04\n",
      "Saved standings for 2004-05\n",
      "Saved standings for 2005-06\n",
      "Saved standings for 2006-07\n",
      "Saved standings for 2007-08\n",
      "Saved standings for 2008-09\n",
      "Saved standings for 2009-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/BRScraper/nba.py:317: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  name2 = re.sub('[^a-zA-Z0-9 \\n\\.]', '', name)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "2010 is not a valid season.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/BRScraper/nba.py:108\u001b[0m, in \u001b[0;36mget_standings\u001b[0;34m(season, info)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/html.py:983\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03mParse and return all tables from the DOM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03mlist of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/html.py:806\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/html.py:785\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio):\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    788\u001b[0m         r \u001b[38;5;241m=\u001b[39m parse(f\u001b[38;5;241m.\u001b[39mhandle, parser\u001b[38;5;241m=\u001b[39mparser)\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/pandas/io/common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/urllib/request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.12/urllib/request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.12/urllib/request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    558\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/urllib/request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1980\u001b[39m, \u001b[38;5;241m2025\u001b[39m):\n\u001b[1;32m      7\u001b[0m     season \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(year\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Format season as '1980-81'\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     standings \u001b[38;5;241m=\u001b[39m \u001b[43mnba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_standings\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     standings\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstandings_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_standings.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved standings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/01_CLASSES/7_fall_2024_semester/csci_39543/venv/lib/python3.12/site-packages/BRScraper/nba.py:110\u001b[0m, in \u001b[0;36mget_standings\u001b[0;34m(season, info)\u001b[0m\n\u001b[1;32m    108\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(url)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(season)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid season.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    113\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: 2010 is not a valid season."
     ]
    }
   ],
   "source": [
    "# Define the folder for standings data\n",
    "standings_folder = \"nba_season_standings\"\n",
    "os.makedirs(standings_folder, exist_ok=True)\n",
    "\n",
    "# Scrape and save standings data for each season\n",
    "for year in range(1980, 2025):\n",
    "    season = f\"{year}-{str(year + 1)[-2:]}\"  # Format season as '1980-81'\n",
    "    standings = leaguestandings.LeagueStandings\n",
    "    standings.to_csv(f\"{standings_folder}/{season}_standings.csv\", index=False)\n",
    "    print(f\"Saved standings for {season}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
